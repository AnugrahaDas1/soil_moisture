{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0054343",
   "metadata": {},
   "source": [
    "# Multi-Temporal Satellite Band Extraction for Soil Moisture Analysis\n",
    "\n",
    "This notebook extracts raw bands from Sentinel-1 and Sentinel-2 satellites for soil moisture sampling points using a multi-temporal approach:\n",
    "\n",
    "## Methodology:\n",
    "1. **Temporal Strategy**: Extract data from 3 time periods per point:\n",
    "   - Same date as field measurement\n",
    "   - One image before (within search window)\n",
    "   - One image after (within search window)\n",
    "\n",
    "2. **Sentinel-2**: Extract raw bands (B2, B3, B4, B8, B11, B12)\n",
    "   - Cloud pixels remain as null/empty values\n",
    "   - No cloud masking replacement\n",
    "\n",
    "3. **Sentinel-1**: Extract VV, VH backscatter + incidence angle\n",
    "   - Include orbit direction (ascending/descending)\n",
    "   - Include acquisition metadata\n",
    "\n",
    "4. **Output**: Tidy dataset ready for spectral index calculation and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27403417",
   "metadata": {},
   "source": [
    "## 1. Set Up Python and Google Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fceeca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Google Earth Engine initialized successfully\n",
      "✓ Connection test: Found 1 image(s)\n",
      "✓ Setup complete - Ready for band extraction!\n",
      "✓ Connection test: Found 1 image(s)\n",
      "✓ Setup complete - Ready for band extraction!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import ee\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Authenticate and initialize Google Earth Engine\n",
    "try:\n",
    "    ee.Initialize()\n",
    "    print(\"✓ Google Earth Engine initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(\"⚠ Please authenticate Google Earth Engine first:\")\n",
    "    print(\"Run: ee.Authenticate()\")\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "    print(\"✓ Google Earth Engine initialized successfully\")\n",
    "\n",
    "# Test connection\n",
    "test_collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED').limit(1)\n",
    "print(f\"✓ Connection test: Found {test_collection.size().getInfo()} image(s)\")\n",
    "print(f\"✓ Setup complete - Ready for band extraction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8356c1",
   "metadata": {},
   "source": [
    "## 2. Load Point Dataset with Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a44b5168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET OVERVIEW ===\n",
      "Total points: 23\n",
      "Columns: ['fid_1', 'N', 'S', 'W', 'E', 'C', 'Date', 'geometry']\n",
      "Date range: 2025-06-29 00:00:00 to 2025-09-27 00:00:00\n",
      "Unique dates: 2\n",
      "\n",
      "=== POINTS BY DATE ===\n",
      "  2025-06-29: 10 points\n",
      "  2025-09-27: 13 points\n",
      "\n",
      "✓ Created GEE FeatureCollection with 23 points\n",
      "\n",
      "=== SAMPLE POINTS ===\n",
      "\n",
      "✓ Created GEE FeatureCollection with 23 points\n",
      "\n",
      "=== SAMPLE POINTS ===\n",
      "  Point 0: (9.96521, 49.77770) - 2025-06-29\n",
      "  Point 1: (9.96676, 49.77433) - 2025-06-29\n",
      "  Point 2: (9.96504, 49.77385) - 2025-06-29\n",
      "  Point 0: (9.96521, 49.77770) - 2025-06-29\n",
      "  Point 1: (9.96676, 49.77433) - 2025-06-29\n",
      "  Point 2: (9.96504, 49.77385) - 2025-06-29\n"
     ]
    }
   ],
   "source": [
    "# Load the shapefile with sampling points\n",
    "shapefile_path = \"soil_moisture_sample_points_7sep.shp\"\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Total points: {len(gdf)}\")\n",
    "print(f\"Columns: {list(gdf.columns)}\")\n",
    "print(f\"Date range: {gdf['Date'].min()} to {gdf['Date'].max()}\")\n",
    "print(f\"Unique dates: {gdf['Date'].nunique()}\")\n",
    "\n",
    "# Ensure dates are properly formatted\n",
    "gdf['Date_dt'] = pd.to_datetime(gdf['Date'])\n",
    "\n",
    "# Display summary by date\n",
    "print(\"\\n=== POINTS BY DATE ===\")\n",
    "date_summary = gdf.groupby('Date').size()\n",
    "for date, count in date_summary.items():\n",
    "    print(f\"  {date.strftime('%Y-%m-%d')}: {count} points\")\n",
    "\n",
    "# Convert to GEE FeatureCollection with essential properties\n",
    "gee_features = []\n",
    "for idx, row in gdf.iterrows():\n",
    "    # Create point geometry\n",
    "    point = ee.Geometry.Point([row.geometry.x, row.geometry.y])\n",
    "    \n",
    "    # Create feature with essential properties\n",
    "    feature = ee.Feature(point, {\n",
    "        'point_id': int(row['fid_1']),           # Point identifier\n",
    "        'point_date': row['Date_dt'].strftime('%Y-%m-%d'),  # Sampling date as string\n",
    "        'longitude': float(row.geometry.x),      # For reference\n",
    "        'latitude': float(row.geometry.y)        # For reference\n",
    "    })\n",
    "    gee_features.append(feature)\n",
    "\n",
    "# Create feature collection\n",
    "points_collection = ee.FeatureCollection(gee_features)\n",
    "print(f\"\\n✓ Created GEE FeatureCollection with {points_collection.size().getInfo()} points\")\n",
    "\n",
    "# Show first few points for verification\n",
    "print(\"\\n=== SAMPLE POINTS ===\")\n",
    "sample_points = points_collection.limit(3).getInfo()['features']\n",
    "for i, point in enumerate(sample_points):\n",
    "    props = point['properties']\n",
    "    coords = point['geometry']['coordinates']\n",
    "    print(f\"  Point {props['point_id']}: ({coords[0]:.5f}, {coords[1]:.5f}) - {props['point_date']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb6888c",
   "metadata": {},
   "source": [
    "## 3. Configure Search Parameters and AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b14ff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXTRACTION CONFIGURATION ===\n",
      "  max_days_before: 10\n",
      "  max_days_after: 10\n",
      "  s2_bands: ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']\n",
      "  s1_bands: ['VV', 'VH', 'angle']\n",
      "  s2_scale: 10\n",
      "  s1_scale: 10\n",
      "  aoi_buffer: 9e-05\n",
      "  s1_speckle_filter_size: 30\n",
      "  s1_angle_min: 30.64\n",
      "  s1_angle_max: 45.24\n",
      "\n",
      "✓ AOI created with 9e-05 degree buffer\n",
      "✓ Date utility functions defined\n",
      "✓ Configuration complete - Ready for image collection setup\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    # Temporal search windows\n",
    "    'max_days_before': 10,  # Maximum days to look for previous image\n",
    "    'max_days_after': 10,   # Maximum days to look for next image\n",
    "    \n",
    "    # Sentinel-2 bands to extract\n",
    "    's2_bands': ['B2', 'B3', 'B4', 'B8', 'B11', 'B12'],  # Blue, Green, Red, NIR, SWIR1, SWIR2\n",
    "    \n",
    "    # Sentinel-1 bands to extract\n",
    "    's1_bands': ['VV', 'VH', 'angle'],  # VV, VH polarizations + incidence angle\n",
    "    \n",
    "    # Spatial resolution for sampling\n",
    "    's2_scale': 10,  # meters\n",
    "    's1_scale': 10,  # meters\n",
    "    \n",
    "    # Area of Interest buffer\n",
    "    'aoi_buffer': 0.00009,  # degrees (~10m buffer around points)\n",
    "    \n",
    "    # Sentinel-1 preprocessing parameters\n",
    "    's1_speckle_filter_size': 30,  # meters - for speckle filtering\n",
    "    's1_angle_min': 30.64,  # degrees - minimum incidence angle (border noise removal)\n",
    "    's1_angle_max': 45.24   # degrees - maximum incidence angle (border noise removal)\n",
    "}\n",
    "\n",
    "print(\"=== EXTRACTION CONFIGURATION ===\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create Area of Interest (AOI) from points with buffer\n",
    "aoi = points_collection.geometry().bounds().buffer(CONFIG['aoi_buffer'])\n",
    "\n",
    "print(f\"\\n✓ AOI created with {CONFIG['aoi_buffer']} degree buffer\")\n",
    "\n",
    "# Define utility functions for date handling\n",
    "def get_date_range(center_date_str, days_before=0, days_after=0):\n",
    "    \"\"\"Get date range for filtering collections\"\"\"\n",
    "    center_date = datetime.strptime(center_date_str, '%Y-%m-%d')\n",
    "    start_date = center_date - timedelta(days=days_before)\n",
    "    end_date = center_date + timedelta(days=days_after + 1)  # +1 for inclusive end\n",
    "    return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "def get_same_day_range(date_str):\n",
    "    \"\"\"Get same day range (date to date+1)\"\"\"\n",
    "    return get_date_range(date_str, 0, 0)\n",
    "\n",
    "def get_before_range(date_str, max_days):\n",
    "    \"\"\"Get range for previous images\"\"\"\n",
    "    center_date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    start_date = center_date - timedelta(days=max_days)\n",
    "    end_date = center_date  # Up to but not including center date\n",
    "    return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "def get_after_range(date_str, max_days):\n",
    "    \"\"\"Get range for next images\"\"\"\n",
    "    center_date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    start_date = center_date + timedelta(days=1)  # After center date\n",
    "    end_date = center_date + timedelta(days=max_days + 1)\n",
    "    return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "print(\"✓ Date utility functions defined\")\n",
    "print(\"✓ Configuration complete - Ready for image collection setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f365f05f",
   "metadata": {},
   "source": [
    "## 4. Build Sentinel-2 Collection (Cloud-Aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "565b5b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SENTINEL-2 COLLECTION SETUP ===\n",
      "Date range: 2025-06-19 to 2025-10-07\n",
      "✓ Base S2 collection: 62 images\n",
      "✓ Cloud masking applied - cloudy pixels will return null values\n",
      "✓ Sentinel-2 collection ready for temporal extraction\n",
      "\n",
      "=== S2 AVAILABILITY CHECK ===\n",
      "✓ Base S2 collection: 62 images\n",
      "✓ Cloud masking applied - cloudy pixels will return null values\n",
      "✓ Sentinel-2 collection ready for temporal extraction\n",
      "\n",
      "=== S2 AVAILABILITY CHECK ===\n",
      "  2025-06-29: 1 image(s) available\n",
      "  2025-06-29: 1 image(s) available\n",
      "  2025-09-27: 2 image(s) available\n",
      "  2025-09-27: 2 image(s) available\n"
     ]
    }
   ],
   "source": [
    "# Build Sentinel-2 collection\n",
    "# Get overall date range from points\n",
    "min_date = gdf['Date_dt'].min() - timedelta(days=CONFIG['max_days_before'])\n",
    "max_date = gdf['Date_dt'].max() + timedelta(days=CONFIG['max_days_after'])\n",
    "\n",
    "print(\"=== SENTINEL-2 COLLECTION SETUP ===\")\n",
    "print(f\"Date range: {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Create base Sentinel-2 collection (include QA60 for cloud masking)\n",
    "s2_collection = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "                 .filterBounds(aoi)\n",
    "                 .filterDate(min_date.strftime('%Y-%m-%d'), max_date.strftime('%Y-%m-%d'))\n",
    "                 .select(CONFIG['s2_bands'] + ['QA60']))\n",
    "\n",
    "print(f\"✓ Base S2 collection: {s2_collection.size().getInfo()} images\")\n",
    "\n",
    "def apply_cloud_mask_s2(image):\n",
    "    \"\"\"\n",
    "    Apply cloud mask to Sentinel-2 image but keep masked pixels as null\n",
    "    This ensures cloudy pixels return null values when sampled\n",
    "    \"\"\"\n",
    "    # Cloud mask using QA60 band\n",
    "    qa = image.select('QA60')\n",
    "    \n",
    "    # Bits 10 and 11 are clouds and cirrus, respectively\n",
    "    cloud_bit_mask = 1 << 10\n",
    "    cirrus_bit_mask = 1 << 11\n",
    "    \n",
    "    # Create mask (1 for clear pixels, 0 for clouds)\n",
    "    mask = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(\n",
    "           qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
    "    \n",
    "    # Apply mask to selected bands and keep only the bands we want\n",
    "    return image.select(CONFIG['s2_bands']).updateMask(mask).copyProperties(image, [\"system:time_start\", \"system:id\"])\n",
    "\n",
    "# Apply cloud masking to collection\n",
    "s2_masked = s2_collection.map(apply_cloud_mask_s2)\n",
    "\n",
    "print(\"✓ Cloud masking applied - cloudy pixels will return null values\")\n",
    "print(\"✓ Sentinel-2 collection ready for temporal extraction\")\n",
    "\n",
    "# Test collection availability\n",
    "print(f\"\\n=== S2 AVAILABILITY CHECK ===\")\n",
    "# Check a few sample dates\n",
    "sample_dates = gdf['Date_dt'].unique()[:3]\n",
    "for sample_date in sample_dates:\n",
    "    date_str = sample_date.strftime('%Y-%m-%d')\n",
    "    start_str, end_str = get_same_day_range(date_str)\n",
    "    same_day = s2_masked.filterDate(start_str, end_str)\n",
    "    print(f\"  {date_str}: {same_day.size().getInfo()} image(s) available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c982b7b",
   "metadata": {},
   "source": [
    "## 5. Build Sentinel-1 Collection with Orbit/Angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd54c66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SENTINEL-1 COLLECTION SETUP WITH PREPROCESSING ===\n",
      "✓ Base S1 collection: 69 images\n",
      "✓ Applying border noise removal...\n",
      "✓ Applying speckle filtering...\n",
      "✓ Adding orbit metadata...\n",
      "✓ Base S1 collection: 69 images\n",
      "✓ Applying border noise removal...\n",
      "✓ Applying speckle filtering...\n",
      "✓ Adding orbit metadata...\n",
      "✓ Preprocessed S1 collection ready: 69 images\n",
      "  - Border noise removed (angles: 30.6° - 45.2°)\n",
      "  - Speckle filtered (kernel size: 30m)\n",
      "\n",
      "=== S1 AVAILABILITY CHECK ===\n",
      "✓ Preprocessed S1 collection ready: 69 images\n",
      "  - Border noise removed (angles: 30.6° - 45.2°)\n",
      "  - Speckle filtered (kernel size: 30m)\n",
      "\n",
      "=== S1 AVAILABILITY CHECK ===\n",
      "  2025-06-29: 1 image(s) - Orbit: Unknown\n",
      "  2025-06-29: 1 image(s) - Orbit: Unknown\n",
      "  2025-09-27: 1 image(s) - Orbit: Unknown\n",
      "✓ Sentinel-1 collection ready for temporal extraction\n",
      "  2025-09-27: 1 image(s) - Orbit: Unknown\n",
      "✓ Sentinel-1 collection ready for temporal extraction\n"
     ]
    }
   ],
   "source": [
    "# Build Sentinel-1 collection with preprocessing\n",
    "print(\"=== SENTINEL-1 COLLECTION SETUP WITH PREPROCESSING ===\")\n",
    "\n",
    "# Create base Sentinel-1 collection\n",
    "s1_collection = (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
    "                 .filterBounds(aoi)\n",
    "                 .filterDate(min_date.strftime('%Y-%m-%d'), max_date.strftime('%Y-%m-%d'))\n",
    "                 .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
    "                 .filter(ee.Filter.eq('resolution_meters', 10))\n",
    "                 .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
    "                 .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
    "                 .select(CONFIG['s1_bands']))\n",
    "\n",
    "print(f\"✓ Base S1 collection: {s1_collection.size().getInfo()} images\")\n",
    "\n",
    "# Define Sentinel-1 preprocessing functions\n",
    "def apply_border_noise_removal(image):\n",
    "    \"\"\"\n",
    "    Remove border noise by masking pixels with extreme incidence angles\n",
    "    This removes edge effects that can contaminate SAR data\n",
    "    \"\"\"\n",
    "    angle = image.select('angle')\n",
    "    \n",
    "    # Create mask for valid incidence angles (exclude edges)\n",
    "    valid_angle_mask = angle.gt(CONFIG['s1_angle_min']).And(angle.lt(CONFIG['s1_angle_max']))\n",
    "    \n",
    "    # Apply mask to all bands\n",
    "    return image.updateMask(valid_angle_mask).copyProperties(image, ['system:time_start', 'system:id'])\n",
    "\n",
    "def apply_speckle_filter(image):\n",
    "    \"\"\"\n",
    "    Apply median filter to reduce speckle noise in SAR data\n",
    "    This improves data quality for point-based sampling\n",
    "    \"\"\"\n",
    "    # Apply focal median filter to VV and VH bands\n",
    "    vv_filtered = image.select('VV').focal_median(CONFIG['s1_speckle_filter_size'], 'square', 'meters')\n",
    "    vh_filtered = image.select('VH').focal_median(CONFIG['s1_speckle_filter_size'], 'square', 'meters')\n",
    "    \n",
    "    # Keep the angle band unchanged and combine with filtered polarizations\n",
    "    filtered = ee.Image.cat([\n",
    "        vv_filtered.rename('VV'),\n",
    "        vh_filtered.rename('VH'),\n",
    "        image.select('angle')\n",
    "    ])\n",
    "    \n",
    "    return filtered.copyProperties(image, ['system:time_start', 'system:id'])\n",
    "\n",
    "def add_orbit_properties(image):\n",
    "    \"\"\"Add orbit direction and other metadata as properties\"\"\"\n",
    "    # Get orbit properties\n",
    "    orbit_pass = image.get('orbitProperties_pass')  # ASCENDING or DESCENDING\n",
    "    relative_orbit = image.get('relativeOrbitNumber_start')\n",
    "    \n",
    "    # Add as image properties (will be extracted during sampling)\n",
    "    return image.set({\n",
    "        'orbit_direction': orbit_pass,\n",
    "        'relative_orbit': relative_orbit,\n",
    "        'acquisition_date': ee.Date(image.get('system:time_start')).format('YYYY-MM-dd')\n",
    "    })\n",
    "\n",
    "# Apply preprocessing pipeline\n",
    "print(\"✓ Applying border noise removal...\")\n",
    "s1_border_cleaned = s1_collection.map(apply_border_noise_removal)\n",
    "\n",
    "print(\"✓ Applying speckle filtering...\")\n",
    "s1_speckle_filtered = s1_border_cleaned.map(apply_speckle_filter)\n",
    "\n",
    "print(\"✓ Adding orbit metadata...\")\n",
    "s1_with_metadata = s1_speckle_filtered.map(add_orbit_properties)\n",
    "\n",
    "print(f\"✓ Preprocessed S1 collection ready: {s1_with_metadata.size().getInfo()} images\")\n",
    "print(f\"  - Border noise removed (angles: {CONFIG['s1_angle_min']:.1f}° - {CONFIG['s1_angle_max']:.1f}°)\")\n",
    "print(f\"  - Speckle filtered (kernel size: {CONFIG['s1_speckle_filter_size']}m)\")\n",
    "\n",
    "# Test S1 collection availability\n",
    "print(f\"\\n=== S1 AVAILABILITY CHECK ===\")\n",
    "for sample_date in sample_dates:\n",
    "    date_str = sample_date.strftime('%Y-%m-%d')\n",
    "    start_str, end_str = get_same_day_range(date_str)\n",
    "    same_day = s1_with_metadata.filterDate(start_str, end_str)\n",
    "    \n",
    "    if same_day.size().getInfo() > 0:\n",
    "        # Get orbit info for available image\n",
    "        first_image = same_day.first()\n",
    "        orbit_info = first_image.getInfo()['properties']\n",
    "        orbit_dir = orbit_info.get('orbit_direction', 'Unknown')\n",
    "        print(f\"  {date_str}: {same_day.size().getInfo()} image(s) - Orbit: {orbit_dir}\")\n",
    "    else:\n",
    "        print(f\"  {date_str}: No images available\")\n",
    "\n",
    "print(\"✓ Sentinel-1 collection ready for temporal extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3f25c3",
   "metadata": {},
   "source": [
    "## Sentinel-1 Preprocessing Applied\n",
    "\n",
    "The following preprocessing steps have been applied to improve Sentinel-1 data quality:\n",
    "\n",
    "### ✅ **Border Noise Removal:**\n",
    "- **Purpose**: Eliminates edge effects and unreliable pixels at image borders\n",
    "- **Method**: Masks pixels with incidence angles outside 30.6° - 45.2°\n",
    "- **Benefit**: Prevents sampling of corrupted edge pixels\n",
    "\n",
    "### ✅ **Speckle Filtering:**\n",
    "- **Purpose**: Reduces inherent SAR speckle noise \n",
    "- **Method**: Focal median filter with 30m kernel\n",
    "- **Benefit**: Improves reliability of point-based measurements\n",
    "\n",
    "### ⏭️ **Skipped Corrections:**\n",
    "- **Terrain Correction**: Not needed - all points at similar elevation\n",
    "- **Orbit Filtering**: Maintained diversity for better temporal coverage\n",
    "\n",
    "This preprocessing significantly improves data quality while preserving the multi-temporal approach needed for your soil moisture analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa53957e",
   "metadata": {},
   "source": [
    "## 6. Multi-Temporal Image Selection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ee3eccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Multi-temporal extraction functions defined\n",
      "✓ Ready for band sampling at each point\n"
     ]
    }
   ],
   "source": [
    "def get_temporal_images(collection, point_date, max_days_before, max_days_after):\n",
    "    \"\"\"\n",
    "    Get same-day, previous, and next images for a given point date\n",
    "    Returns dictionary with 'same', 'previous', 'next' keys containing image IDs or None\n",
    "    \"\"\"\n",
    "    results = {'same': None, 'previous': None, 'next': None}\n",
    "    \n",
    "    # Same day images\n",
    "    start_same, end_same = get_same_day_range(point_date)\n",
    "    same_day_collection = collection.filterDate(start_same, end_same)\n",
    "    if same_day_collection.size().getInfo() > 0:\n",
    "        results['same'] = same_day_collection.first().get('system:id')\n",
    "    \n",
    "    # Previous images (before point date)\n",
    "    start_prev, end_prev = get_before_range(point_date, max_days_before)\n",
    "    prev_collection = collection.filterDate(start_prev, end_prev).sort('system:time_start', False)\n",
    "    if prev_collection.size().getInfo() > 0:\n",
    "        results['previous'] = prev_collection.first().get('system:id')  # Most recent before\n",
    "    \n",
    "    # Next images (after point date)  \n",
    "    start_next, end_next = get_after_range(point_date, max_days_after)\n",
    "    next_collection = collection.filterDate(start_next, end_next).sort('system:time_start')\n",
    "    if next_collection.size().getInfo() > 0:\n",
    "        results['next'] = next_collection.first().get('system:id')  # Earliest after\n",
    "    \n",
    "    return results\n",
    "\n",
    "def sample_bands_at_point(image, point_geometry, bands, scale, sensor_type):\n",
    "    \"\"\"\n",
    "    Sample bands at a point location and return properties\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    # Sample the image at point location\n",
    "    sample = image.sampleRegions(\n",
    "        collection=ee.FeatureCollection([ee.Feature(point_geometry)]),\n",
    "        scale=scale,\n",
    "        projection='EPSG:4326'\n",
    "    ).first()\n",
    "    \n",
    "    # Get sampled values - handle case where sampling returns None\n",
    "    sample_info = sample.getInfo()\n",
    "    if sample_info is None or 'properties' not in sample_info:\n",
    "        print(f\"    Warning: No valid data at point for {sensor_type}\")\n",
    "        return None\n",
    "    \n",
    "    sampled_data = sample_info['properties']\n",
    "    \n",
    "    # Get image metadata\n",
    "    image_props = image.getInfo()['properties']\n",
    "    \n",
    "    # Combine band values with metadata\n",
    "    result = {}\n",
    "    \n",
    "    # Add band values\n",
    "    for band in bands:\n",
    "        result[f'{sensor_type.lower()}_{band}'] = sampled_data.get(band, None)\n",
    "    \n",
    "    # Add metadata\n",
    "    result['image_id'] = image_props.get('system:id', '')\n",
    "    result['acquisition_date'] = image_props.get('acquisition_date', '')\n",
    "    \n",
    "    # Add sensor-specific metadata\n",
    "    if sensor_type == 'S1':\n",
    "        result['s1_orbit_direction'] = image_props.get('orbit_direction', '')\n",
    "        result['s1_relative_orbit'] = image_props.get('relative_orbit', '')\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✓ Multi-temporal extraction functions defined\")\n",
    "print(\"✓ Ready for band sampling at each point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16ea929",
   "metadata": {},
   "source": [
    "## 7. Extract Sentinel-2 Bands (Multi-Temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51242a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXTRACTING SENTINEL-2 BANDS ===\n",
      "Note: Cloud pixels will return null values (not masked/replaced)\n",
      "Processing point 0 (1/23) - Date: 2025-06-29\n",
      "Processing point 0 (1/23) - Date: 2025-06-29\n",
      "  same: Clear data - \n",
      "  same: Clear data - \n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: Clear data - \n",
      "Processing point 1 (2/23) - Date: 2025-06-29\n",
      "  next: Clear data - \n",
      "Processing point 1 (2/23) - Date: 2025-06-29\n",
      "  same: Clear data - \n",
      "  same: Clear data - \n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: Clear data - \n",
      "Processing point 2 (3/23) - Date: 2025-06-29\n",
      "  next: Clear data - \n",
      "Processing point 2 (3/23) - Date: 2025-06-29\n",
      "  same: Clear data - \n",
      "  same: Clear data - \n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: Clear data - \n",
      "Processing point 4 (4/23) - Date: 2025-06-29\n",
      "  next: Clear data - \n",
      "Processing point 4 (4/23) - Date: 2025-06-29\n",
      "  same: Clear data - \n",
      "  same: Clear data - \n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: Clear data - \n",
      "Processing point 6 (5/23) - Date: 2025-06-29\n",
      "  next: Clear data - \n",
      "Processing point 6 (5/23) - Date: 2025-06-29\n",
      "  same: Clear data - \n",
      "  same: Clear data - \n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: Clear data - \n",
      "Processing point 7 (6/23) - Date: 2025-06-29\n",
      "  next: Clear data - \n",
      "Processing point 7 (6/23) - Date: 2025-06-29\n",
      "  same: Clear data - \n",
      "  same: Clear data - \n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: Clear data - \n",
      "Processing point 8 (7/23) - Date: 2025-06-29\n",
      "  next: Clear data - \n",
      "Processing point 8 (7/23) - Date: 2025-06-29\n",
      "  same: Clear data - \n",
      "  same: Clear data - \n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: Clear data - \n",
      "Processing point 9 (8/23) - Date: 2025-06-29\n",
      "  next: Clear data - \n",
      "Processing point 9 (8/23) - Date: 2025-06-29\n",
      "  same: Clear data - \n",
      "  same: Clear data - \n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: Clear data - \n",
      "Processing point 16 (9/23) - Date: 2025-09-27\n",
      "  next: Clear data - \n",
      "Processing point 16 (9/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 19 (10/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 19 (10/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 20 (11/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 20 (11/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 22 (12/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 22 (12/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 31 (13/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 31 (13/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 33 (14/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 33 (14/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 70 (15/23) - Date: 2025-06-29\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 70 (15/23) - Date: 2025-06-29\n",
      "  same: Clear data - \n",
      "  same: Clear data - \n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: Clear data - \n",
      "Processing point 71 (16/23) - Date: 2025-06-29\n",
      "  next: Clear data - \n",
      "Processing point 71 (16/23) - Date: 2025-06-29\n",
      "  same: Clear data - \n",
      "  same: Clear data - \n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: Clear data - \n",
      "Processing point 80 (17/23) - Date: 2025-09-27\n",
      "  next: Clear data - \n",
      "Processing point 80 (17/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 81 (18/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 81 (18/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 82 (19/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 82 (19/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 83 (20/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 83 (20/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 84 (21/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 84 (21/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 85 (22/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 85 (22/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 86 (23/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "Processing point 86 (23/23) - Date: 2025-09-27\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "\n",
      "✓ Sentinel-2 extraction complete: 20 records\n",
      "✓ S2 DataFrame created: (20, 14)\n",
      "  Columns: ['s2_B2', 's2_B3', 's2_B4', 's2_B8', 's2_B11', 's2_B12', 'image_id', 'acquisition_date', 'point_id', 'point_date', 'slot', 'sensor', 'longitude', 'latitude']\n",
      "\n",
      "=== CLOUD IMPACT SUMMARY ===\n",
      "  s2_B2: 0/20 cloudy (0.0%)\n",
      "  s2_B3: 0/20 cloudy (0.0%)\n",
      "  s2_B4: 0/20 cloudy (0.0%)\n",
      "  s2_B8: 0/20 cloudy (0.0%)\n",
      "  s2_B11: 0/20 cloudy (0.0%)\n",
      "  s2_B12: 0/20 cloudy (0.0%)\n",
      "    Warning: No valid data at point for S2\n",
      "  next: No image available\n",
      "\n",
      "✓ Sentinel-2 extraction complete: 20 records\n",
      "✓ S2 DataFrame created: (20, 14)\n",
      "  Columns: ['s2_B2', 's2_B3', 's2_B4', 's2_B8', 's2_B11', 's2_B12', 'image_id', 'acquisition_date', 'point_id', 'point_date', 'slot', 'sensor', 'longitude', 'latitude']\n",
      "\n",
      "=== CLOUD IMPACT SUMMARY ===\n",
      "  s2_B2: 0/20 cloudy (0.0%)\n",
      "  s2_B3: 0/20 cloudy (0.0%)\n",
      "  s2_B4: 0/20 cloudy (0.0%)\n",
      "  s2_B8: 0/20 cloudy (0.0%)\n",
      "  s2_B11: 0/20 cloudy (0.0%)\n",
      "  s2_B12: 0/20 cloudy (0.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== EXTRACTING SENTINEL-2 BANDS ===\")\n",
    "print(\"Note: Cloud pixels will return null values (not masked/replaced)\")\n",
    "\n",
    "# Extract Sentinel-2 data for all points\n",
    "s2_results = []\n",
    "\n",
    "# Get point list for processing\n",
    "points_list = points_collection.getInfo()['features']\n",
    "total_points = len(points_list)\n",
    "\n",
    "for i, point_feature in enumerate(points_list):\n",
    "    point_props = point_feature['properties']\n",
    "    point_geom = ee.Geometry.Point(point_feature['geometry']['coordinates'])\n",
    "    point_id = point_props['point_id']\n",
    "    point_date = point_props['point_date']\n",
    "    \n",
    "    print(f\"Processing point {point_id} ({i+1}/{total_points}) - Date: {point_date}\")\n",
    "    \n",
    "    # Get temporal images for this point\n",
    "    temporal_images = get_temporal_images(\n",
    "        s2_masked, point_date, \n",
    "        CONFIG['max_days_before'], \n",
    "        CONFIG['max_days_after']\n",
    "    )\n",
    "    \n",
    "    # Process each temporal slot\n",
    "    for slot, image_id in temporal_images.items():\n",
    "        if image_id is not None:\n",
    "            # Get the actual image\n",
    "            image_id_str = image_id.getInfo()\n",
    "            image = s2_masked.filter(ee.Filter.eq('system:id', image_id_str)).first()\n",
    "            \n",
    "            # Sample bands at point\n",
    "            sampled_data = sample_bands_at_point(\n",
    "                image, point_geom, CONFIG['s2_bands'], CONFIG['s2_scale'], 'S2'\n",
    "            )\n",
    "            \n",
    "            if sampled_data:\n",
    "                # Add point and slot information\n",
    "                sampled_data.update({\n",
    "                    'point_id': point_id,\n",
    "                    'point_date': point_date,\n",
    "                    'slot': slot,\n",
    "                    'sensor': 'S2',\n",
    "                    'longitude': point_props['longitude'],\n",
    "                    'latitude': point_props['latitude']\n",
    "                })\n",
    "                s2_results.append(sampled_data)\n",
    "                \n",
    "                # Show sample extraction info\n",
    "                cloud_bands = [band for band in CONFIG['s2_bands'] \n",
    "                             if sampled_data.get(f's2_{band}') is None]\n",
    "                if cloud_bands:\n",
    "                    print(f\"  {slot}: {len(cloud_bands)} cloudy bands - {sampled_data['acquisition_date']}\")\n",
    "                else:\n",
    "                    print(f\"  {slot}: Clear data - {sampled_data['acquisition_date']}\")\n",
    "        else:\n",
    "            print(f\"  {slot}: No image available\")\n",
    "\n",
    "print(f\"\\n✓ Sentinel-2 extraction complete: {len(s2_results)} records\")\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "s2_df = pd.DataFrame(s2_results)\n",
    "if len(s2_df) > 0:\n",
    "    print(f\"✓ S2 DataFrame created: {s2_df.shape}\")\n",
    "    print(f\"  Columns: {list(s2_df.columns)}\")\n",
    "    \n",
    "    # Summary of cloud coverage\n",
    "    s2_band_cols = [col for col in s2_df.columns if col.startswith('s2_')]\n",
    "    null_counts = s2_df[s2_band_cols].isnull().sum()\n",
    "    print(f\"\\n=== CLOUD IMPACT SUMMARY ===\")\n",
    "    for band, null_count in null_counts.items():\n",
    "        total_extractions = len(s2_df)\n",
    "        print(f\"  {band}: {null_count}/{total_extractions} cloudy ({null_count/total_extractions*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"⚠ No Sentinel-2 data extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db9b218",
   "metadata": {},
   "source": [
    "## 8. Extract Sentinel-1 Bands with Orbit Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7f68f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXTRACTING SENTINEL-1 BANDS ===\n",
      "Including VV, VH polarizations, incidence angle, and orbit direction\n",
      "Processing point 0 (1/23) - Date: 2025-06-29\n",
      "  same:  orbit, angle: 36.10874557495117° - 2025-06-29\n",
      "  same:  orbit, angle: 36.10874557495117° - 2025-06-29\n",
      "  previous:  orbit, angle: 44.536293029785156° - 2025-06-28\n",
      "  previous:  orbit, angle: 44.536293029785156° - 2025-06-28\n",
      "  next:  orbit, angle: 35.952606201171875° - 2025-07-02\n",
      "Processing point 1 (2/23) - Date: 2025-06-29\n",
      "  next:  orbit, angle: 35.952606201171875° - 2025-07-02\n",
      "Processing point 1 (2/23) - Date: 2025-06-29\n",
      "  same:  orbit, angle: 36.11158752441406° - 2025-06-29\n",
      "  same:  orbit, angle: 36.11158752441406° - 2025-06-29\n",
      "  previous:  orbit, angle: 44.539031982421875° - 2025-06-28\n",
      "  previous:  orbit, angle: 44.539031982421875° - 2025-06-28\n",
      "  next:  orbit, angle: 35.941864013671875° - 2025-07-02\n",
      "Processing point 2 (3/23) - Date: 2025-06-29\n",
      "  next:  orbit, angle: 35.941864013671875° - 2025-07-02\n",
      "Processing point 2 (3/23) - Date: 2025-06-29\n",
      "  same:  orbit, angle: 36.103355407714844° - 2025-06-29\n",
      "  same:  orbit, angle: 36.103355407714844° - 2025-06-29\n",
      "  previous:  orbit, angle: 44.53228759765625° - 2025-06-28\n",
      "  previous:  orbit, angle: 44.53228759765625° - 2025-06-28\n",
      "  next:  orbit, angle: 35.9489860534668° - 2025-07-02\n",
      "Processing point 4 (4/23) - Date: 2025-06-29\n",
      "  next:  orbit, angle: 35.9489860534668° - 2025-07-02\n",
      "Processing point 4 (4/23) - Date: 2025-06-29\n",
      "  same:  orbit, angle: 36.11537551879883° - 2025-06-29\n",
      "  same:  orbit, angle: 36.11537551879883° - 2025-06-29\n",
      "  previous:  orbit, angle: 44.5418701171875° - 2025-06-28\n",
      "  previous:  orbit, angle: 44.5418701171875° - 2025-06-28\n",
      "  next:  orbit, angle: 35.943931579589844° - 2025-07-02\n",
      "Processing point 6 (5/23) - Date: 2025-06-29\n",
      "  next:  orbit, angle: 35.943931579589844° - 2025-07-02\n",
      "Processing point 6 (5/23) - Date: 2025-06-29\n",
      "  same:  orbit, angle: 36.08463668823242° - 2025-06-29\n",
      "  same:  orbit, angle: 36.08463668823242° - 2025-06-29\n",
      "  previous:  orbit, angle: 44.517154693603516° - 2025-06-28\n",
      "  previous:  orbit, angle: 44.517154693603516° - 2025-06-28\n",
      "  next:  orbit, angle: 35.96106719970703° - 2025-07-02\n",
      "Processing point 7 (6/23) - Date: 2025-06-29\n",
      "  next:  orbit, angle: 35.96106719970703° - 2025-07-02\n",
      "Processing point 7 (6/23) - Date: 2025-06-29\n",
      "  same:  orbit, angle: 36.06610107421875° - 2025-06-29\n",
      "  same:  orbit, angle: 36.06610107421875° - 2025-06-29\n",
      "  previous:  orbit, angle: 44.502037048339844° - 2025-06-28\n",
      "  previous:  orbit, angle: 44.502037048339844° - 2025-06-28\n",
      "  next:  orbit, angle: 35.97570037841797° - 2025-07-02\n",
      "Processing point 8 (7/23) - Date: 2025-06-29\n",
      "  next:  orbit, angle: 35.97570037841797° - 2025-07-02\n",
      "Processing point 8 (7/23) - Date: 2025-06-29\n",
      "  same:  orbit, angle: 36.06625747680664° - 2025-06-29\n",
      "  same:  orbit, angle: 36.06625747680664° - 2025-06-29\n",
      "  previous:  orbit, angle: 44.50223159790039° - 2025-06-28\n",
      "  previous:  orbit, angle: 44.50223159790039° - 2025-06-28\n",
      "  next:  orbit, angle: 35.97426986694336° - 2025-07-02\n",
      "Processing point 9 (8/23) - Date: 2025-06-29\n",
      "  next:  orbit, angle: 35.97426986694336° - 2025-07-02\n",
      "Processing point 9 (8/23) - Date: 2025-06-29\n",
      "  same:  orbit, angle: 36.067665100097656° - 2025-06-29\n",
      "  same:  orbit, angle: 36.067665100097656° - 2025-06-29\n",
      "  previous:  orbit, angle: 44.50353240966797° - 2025-06-28\n",
      "  previous:  orbit, angle: 44.50353240966797° - 2025-06-28\n",
      "  next:  orbit, angle: 35.9700813293457° - 2025-07-02\n",
      "Processing point 16 (9/23) - Date: 2025-09-27\n",
      "  next:  orbit, angle: 35.9700813293457° - 2025-07-02\n",
      "Processing point 16 (9/23) - Date: 2025-09-27\n",
      "  same:  orbit, angle: 36.25205993652344° - 2025-09-27\n",
      "  same:  orbit, angle: 36.25205993652344° - 2025-09-27\n",
      "  previous:  orbit, angle: 44.531898498535156° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 19 (10/23) - Date: 2025-09-27\n",
      "  previous:  orbit, angle: 44.531898498535156° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 19 (10/23) - Date: 2025-09-27\n",
      "  same:  orbit, angle: 36.261661529541016° - 2025-09-27\n",
      "  same:  orbit, angle: 36.261661529541016° - 2025-09-27\n",
      "  previous:  orbit, angle: 44.54011154174805° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 20 (11/23) - Date: 2025-09-27\n",
      "  previous:  orbit, angle: 44.54011154174805° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 20 (11/23) - Date: 2025-09-27\n",
      "  same:  orbit, angle: 36.270442962646484° - 2025-09-27\n",
      "  same:  orbit, angle: 36.270442962646484° - 2025-09-27\n",
      "  previous:  orbit, angle: 44.54722213745117° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 22 (12/23) - Date: 2025-09-27\n",
      "  previous:  orbit, angle: 44.54722213745117° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 22 (12/23) - Date: 2025-09-27\n",
      "  same:  orbit, angle: 36.26137924194336° - 2025-09-27\n",
      "  same:  orbit, angle: 36.26137924194336° - 2025-09-27\n",
      "  previous:  orbit, angle: 44.53997039794922° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 31 (13/23) - Date: 2025-09-27\n",
      "  previous:  orbit, angle: 44.53997039794922° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 31 (13/23) - Date: 2025-09-27\n",
      "  same:  orbit, angle: 36.28237533569336° - 2025-09-27\n",
      "  same:  orbit, angle: 36.28237533569336° - 2025-09-27\n",
      "  previous:  orbit, angle: 44.556739807128906° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 33 (14/23) - Date: 2025-09-27\n",
      "  previous:  orbit, angle: 44.556739807128906° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 33 (14/23) - Date: 2025-09-27\n",
      "  same:  orbit, angle: 36.28119659423828° - 2025-09-27\n",
      "  same:  orbit, angle: 36.28119659423828° - 2025-09-27\n",
      "  previous:  orbit, angle: 44.555416107177734° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 70 (15/23) - Date: 2025-06-29\n",
      "  previous:  orbit, angle: 44.555416107177734° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 70 (15/23) - Date: 2025-06-29\n",
      "  same:  orbit, angle: 36.100181579589844° - 2025-06-29\n",
      "  same:  orbit, angle: 36.100181579589844° - 2025-06-29\n",
      "  previous:  orbit, angle: 44.52977752685547° - 2025-06-28\n",
      "  previous:  orbit, angle: 44.52977752685547° - 2025-06-28\n",
      "  next:  orbit, angle: 35.94989013671875° - 2025-07-02\n",
      "Processing point 71 (16/23) - Date: 2025-06-29\n",
      "  next:  orbit, angle: 35.94989013671875° - 2025-07-02\n",
      "Processing point 71 (16/23) - Date: 2025-06-29\n",
      "  same:  orbit, angle: 36.06288528442383° - 2025-06-29\n",
      "  same:  orbit, angle: 36.06288528442383° - 2025-06-29\n",
      "  previous:  orbit, angle: 44.49969482421875° - 2025-06-28\n",
      "  previous:  orbit, angle: 44.49969482421875° - 2025-06-28\n",
      "  next:  orbit, angle: 35.97261428833008° - 2025-07-02\n",
      "Processing point 80 (17/23) - Date: 2025-09-27\n",
      "  next:  orbit, angle: 35.97261428833008° - 2025-07-02\n",
      "Processing point 80 (17/23) - Date: 2025-09-27\n",
      "  same:  orbit, angle: 36.26683044433594° - 2025-09-27\n",
      "  same:  orbit, angle: 36.26683044433594° - 2025-09-27\n",
      "  previous:  orbit, angle: 44.543148040771484° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 81 (18/23) - Date: 2025-09-27\n",
      "  previous:  orbit, angle: 44.543148040771484° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 81 (18/23) - Date: 2025-09-27\n",
      "  same:  orbit, angle: 36.26747512817383° - 2025-09-27\n",
      "  same:  orbit, angle: 36.26747512817383° - 2025-09-27\n",
      "  previous:  orbit, angle: 44.543853759765625° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 82 (19/23) - Date: 2025-09-27\n",
      "  previous:  orbit, angle: 44.543853759765625° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 82 (19/23) - Date: 2025-09-27\n",
      "  same:  orbit, angle: 36.27668380737305° - 2025-09-27\n",
      "  same:  orbit, angle: 36.27668380737305° - 2025-09-27\n",
      "  previous:  orbit, angle: 44.551509857177734° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 83 (20/23) - Date: 2025-09-27\n",
      "  previous:  orbit, angle: 44.551509857177734° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 83 (20/23) - Date: 2025-09-27\n",
      "  same:  orbit, angle: 36.283790588378906° - 2025-09-27\n",
      "  same:  orbit, angle: 36.283790588378906° - 2025-09-27\n",
      "  previous:  orbit, angle: 44.55732345581055° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 84 (21/23) - Date: 2025-09-27\n",
      "  previous:  orbit, angle: 44.55732345581055° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 84 (21/23) - Date: 2025-09-27\n",
      "  same:  orbit, angle: 36.28248977661133° - 2025-09-27\n",
      "  same:  orbit, angle: 36.28248977661133° - 2025-09-27\n",
      "  previous:  orbit, angle: 44.55659484863281° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 85 (22/23) - Date: 2025-09-27\n",
      "  previous:  orbit, angle: 44.55659484863281° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 85 (22/23) - Date: 2025-09-27\n",
      "  same:  orbit, angle: 36.25452423095703° - 2025-09-27\n",
      "  same:  orbit, angle: 36.25452423095703° - 2025-09-27\n",
      "  previous:  orbit, angle: 44.53407287597656° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 86 (23/23) - Date: 2025-09-27\n",
      "  previous:  orbit, angle: 44.53407287597656° - 2025-09-26\n",
      "  next: No image available\n",
      "Processing point 86 (23/23) - Date: 2025-09-27\n",
      "  same:  orbit, angle: 36.2543830871582° - 2025-09-27\n",
      "  same:  orbit, angle: 36.2543830871582° - 2025-09-27\n",
      "  previous:  orbit, angle: 44.53400421142578° - 2025-09-26\n",
      "  next: No image available\n",
      "\n",
      "✓ Sentinel-1 extraction complete: 56 records\n",
      "✓ S1 DataFrame created: (56, 13)\n",
      "  Columns: ['s1_VV', 's1_VH', 's1_angle', 'image_id', 'acquisition_date', 's1_orbit_direction', 's1_relative_orbit', 'point_id', 'point_date', 'slot', 'sensor', 'longitude', 'latitude']\n",
      "\n",
      "=== ORBIT DIRECTION SUMMARY ===\n",
      "  : 56 acquisitions\n",
      "\n",
      "=== INCIDENCE ANGLE SUMMARY ===\n",
      "  Range: 35.9° to 44.6°\n",
      "  Mean: 39.6° ± 4.2°\n",
      "  previous:  orbit, angle: 44.53400421142578° - 2025-09-26\n",
      "  next: No image available\n",
      "\n",
      "✓ Sentinel-1 extraction complete: 56 records\n",
      "✓ S1 DataFrame created: (56, 13)\n",
      "  Columns: ['s1_VV', 's1_VH', 's1_angle', 'image_id', 'acquisition_date', 's1_orbit_direction', 's1_relative_orbit', 'point_id', 'point_date', 'slot', 'sensor', 'longitude', 'latitude']\n",
      "\n",
      "=== ORBIT DIRECTION SUMMARY ===\n",
      "  : 56 acquisitions\n",
      "\n",
      "=== INCIDENCE ANGLE SUMMARY ===\n",
      "  Range: 35.9° to 44.6°\n",
      "  Mean: 39.6° ± 4.2°\n"
     ]
    }
   ],
   "source": [
    "print(\"=== EXTRACTING SENTINEL-1 BANDS ===\")\n",
    "print(\"Including VV, VH polarizations, incidence angle, and orbit direction\")\n",
    "\n",
    "# Extract Sentinel-1 data for all points\n",
    "s1_results = []\n",
    "\n",
    "for i, point_feature in enumerate(points_list):\n",
    "    point_props = point_feature['properties']\n",
    "    point_geom = ee.Geometry.Point(point_feature['geometry']['coordinates'])\n",
    "    point_id = point_props['point_id']\n",
    "    point_date = point_props['point_date']\n",
    "    \n",
    "    print(f\"Processing point {point_id} ({i+1}/{total_points}) - Date: {point_date}\")\n",
    "    \n",
    "    # Get temporal images for this point\n",
    "    temporal_images = get_temporal_images(\n",
    "        s1_with_metadata, point_date, \n",
    "        CONFIG['max_days_before'], \n",
    "        CONFIG['max_days_after']\n",
    "    )\n",
    "    \n",
    "    # Process each temporal slot\n",
    "    for slot, image_id in temporal_images.items():\n",
    "        if image_id is not None:\n",
    "            # Get the actual image\n",
    "            image_id_str = image_id.getInfo()\n",
    "            image = s1_with_metadata.filter(ee.Filter.eq('system:id', image_id_str)).first()\n",
    "            \n",
    "            # Sample bands at point\n",
    "            sampled_data = sample_bands_at_point(\n",
    "                image, point_geom, CONFIG['s1_bands'], CONFIG['s1_scale'], 'S1'\n",
    "            )\n",
    "            \n",
    "            if sampled_data:\n",
    "                # Add point and slot information\n",
    "                sampled_data.update({\n",
    "                    'point_id': point_id,\n",
    "                    'point_date': point_date,\n",
    "                    'slot': slot,\n",
    "                    'sensor': 'S1',\n",
    "                    'longitude': point_props['longitude'],\n",
    "                    'latitude': point_props['latitude']\n",
    "                })\n",
    "                s1_results.append(sampled_data)\n",
    "                \n",
    "                # Show extraction info\n",
    "                orbit_dir = sampled_data.get('s1_orbit_direction', 'Unknown')\n",
    "                inc_angle = sampled_data.get('s1_angle', 'N/A')\n",
    "                print(f\"  {slot}: {orbit_dir} orbit, angle: {inc_angle}° - {sampled_data['acquisition_date']}\")\n",
    "        else:\n",
    "            print(f\"  {slot}: No image available\")\n",
    "\n",
    "print(f\"\\n✓ Sentinel-1 extraction complete: {len(s1_results)} records\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "s1_df = pd.DataFrame(s1_results)\n",
    "if len(s1_df) > 0:\n",
    "    print(f\"✓ S1 DataFrame created: {s1_df.shape}\")\n",
    "    print(f\"  Columns: {list(s1_df.columns)}\")\n",
    "    \n",
    "    # Summary of orbit directions\n",
    "    if 's1_orbit_direction' in s1_df.columns:\n",
    "        orbit_summary = s1_df['s1_orbit_direction'].value_counts()\n",
    "        print(f\"\\n=== ORBIT DIRECTION SUMMARY ===\")\n",
    "        for orbit, count in orbit_summary.items():\n",
    "            print(f\"  {orbit}: {count} acquisitions\")\n",
    "    \n",
    "    # Summary of incidence angles\n",
    "    if 's1_angle' in s1_df.columns:\n",
    "        angles = s1_df['s1_angle'].dropna()\n",
    "        if len(angles) > 0:\n",
    "            print(f\"\\n=== INCIDENCE ANGLE SUMMARY ===\")\n",
    "            print(f\"  Range: {angles.min():.1f}° to {angles.max():.1f}°\")\n",
    "            print(f\"  Mean: {angles.mean():.1f}° ± {angles.std():.1f}°\")\n",
    "else:\n",
    "    print(\"⚠ No Sentinel-1 data extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d1712d",
   "metadata": {},
   "source": [
    "## 9. Merge and Structure Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a98be8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MERGING SENTINEL-1 AND SENTINEL-2 DATA ===\n",
      "✓ Combined dataset: (76, 19)\n",
      "  Total extractions: 76\n",
      "  Sensors: {'S1': 56, 'S2': 20}\n",
      "  Temporal slots: {'same': 33, 'previous': 23, 'next': 20}\n",
      "\n",
      "=== FINAL DATASET STRUCTURE ===\n",
      "Columns (19):\n",
      "   1. point_id\n",
      "   2. point_date\n",
      "   3. slot\n",
      "   4. sensor\n",
      "   5. acquisition_date\n",
      "   6. longitude\n",
      "   7. latitude\n",
      "   8. image_id\n",
      "   9. s2_B2\n",
      "  10. s2_B3\n",
      "  11. s2_B4\n",
      "  12. s2_B8\n",
      "  13. s2_B11\n",
      "  14. s2_B12\n",
      "  15. s1_VV\n",
      "  16. s1_VH\n",
      "  17. s1_angle\n",
      "  18. s1_orbit_direction\n",
      "  19. s1_relative_orbit\n",
      "\n",
      "=== EXTRACTION SUMMARY ===\n",
      "Data availability by sensor and slot:\n",
      "slot    next  previous  same\n",
      "sensor                      \n",
      "S1        10        23    23\n",
      "S2        10         0    10\n",
      "\n",
      "=== SAMPLE DATA (First 5 rows) ===\n",
      " point_id point_date slot sensor acquisition_date  s2_B2  s1_VV s1_orbit_direction\n",
      "        0 2025-06-29 same     S2                   476.0    NaN                NaN\n",
      "        0 2025-06-29 next     S2                   872.0    NaN                NaN\n",
      "        1 2025-06-29 same     S2                   442.0    NaN                NaN\n",
      "        1 2025-06-29 next     S2                   494.0    NaN                NaN\n",
      "        2 2025-06-29 same     S2                   406.0    NaN                NaN\n"
     ]
    }
   ],
   "source": [
    "print(\"=== MERGING SENTINEL-1 AND SENTINEL-2 DATA ===\")\n",
    "\n",
    "# Combine S1 and S2 results\n",
    "all_results = []\n",
    "\n",
    "# Add S2 data\n",
    "if len(s2_df) > 0:\n",
    "    all_results.extend(s2_results)\n",
    "\n",
    "# Add S1 data\n",
    "if len(s1_df) > 0:\n",
    "    all_results.extend(s1_results)\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    # Create comprehensive DataFrame\n",
    "    final_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    print(f\"✓ Combined dataset: {final_df.shape}\")\n",
    "    print(f\"  Total extractions: {len(final_df)}\")\n",
    "    print(f\"  Sensors: {final_df['sensor'].value_counts().to_dict()}\")\n",
    "    print(f\"  Temporal slots: {final_df['slot'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    base_columns = ['point_id', 'point_date', 'slot', 'sensor', 'acquisition_date', \n",
    "                   'longitude', 'latitude', 'image_id']\n",
    "    \n",
    "    s2_columns = [col for col in final_df.columns if col.startswith('s2_')]\n",
    "    s1_columns = [col for col in final_df.columns if col.startswith('s1_')]\n",
    "    \n",
    "    column_order = base_columns + s2_columns + s1_columns\n",
    "    column_order = [col for col in column_order if col in final_df.columns]\n",
    "    \n",
    "    final_df = final_df[column_order]\n",
    "    \n",
    "    print(f\"\\n=== FINAL DATASET STRUCTURE ===\")\n",
    "    print(f\"Columns ({len(final_df.columns)}):\")\n",
    "    for i, col in enumerate(final_df.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n=== EXTRACTION SUMMARY ===\")\n",
    "    print(\"Data availability by sensor and slot:\")\n",
    "    summary = final_df.groupby(['sensor', 'slot']).size().unstack(fill_value=0)\n",
    "    print(summary)\n",
    "    \n",
    "    # Show sample of final data\n",
    "    print(f\"\\n=== SAMPLE DATA (First 5 rows) ===\")\n",
    "    display_cols = ['point_id', 'point_date', 'slot', 'sensor', 'acquisition_date']\n",
    "    if 's2_B2' in final_df.columns:\n",
    "        display_cols.append('s2_B2')\n",
    "    if 's1_VV' in final_df.columns:\n",
    "        display_cols.append('s1_VV')\n",
    "    if 's1_orbit_direction' in final_df.columns:\n",
    "        display_cols.append('s1_orbit_direction')\n",
    "    \n",
    "    sample_data = final_df[display_cols].head()\n",
    "    print(sample_data.to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ No data to merge - check extraction steps\")\n",
    "    final_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5451c617",
   "metadata": {},
   "source": [
    "## 10. Export Results and Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4df8d204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPORTING RESULTS ===\n",
      "✓ Main dataset exported to: multi_temporal_satellite_bands.csv\n",
      "✓ Sentinel-2 data exported to: sentinel2_bands_multi_temporal.csv\n",
      "✓ Sentinel-1 data exported to: sentinel1_bands_multi_temporal.csv\n",
      "\n",
      "=== EXTRACTION REPORT ===\n",
      "Extraction completed: 2025-09-28 22:02:00\n",
      "Total sampling points: 23\n",
      "Total extractions: 76\n",
      "Configuration used:\n",
      "  max_days_before: 10\n",
      "  max_days_after: 10\n",
      "  s2_bands: ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']\n",
      "  s1_bands: ['VV', 'VH', 'angle']\n",
      "  s2_scale: 10\n",
      "  s1_scale: 10\n",
      "  aoi_buffer: 9e-05\n",
      "  s1_speckle_filter_size: 30\n",
      "  s1_angle_min: 30.64\n",
      "  s1_angle_max: 45.24\n",
      "\n",
      "=== DATA QUALITY SUMMARY ===\n",
      "Points with complete temporal coverage: 10\n",
      "\n",
      "Sentinel-2 cloud impact (% null values):\n",
      "  s2_B2: 0.0%\n",
      "  s2_B3: 0.0%\n",
      "  s2_B4: 0.0%\n",
      "  s2_B8: 0.0%\n",
      "  s2_B11: 0.0%\n",
      "  s2_B12: 0.0%\n",
      "\n",
      "Sentinel-1 orbit distribution:\n",
      "  : 56 acquisitions\n",
      "\n",
      "=== NEXT STEPS ===\n",
      "✓ Raw bands extracted and ready for analysis\n",
      "✓ Use extracted bands to calculate spectral indices\n",
      "✓ Perform temporal analysis (same vs previous vs next)\n",
      "✓ Correlate with field soil moisture measurements\n",
      "✓ Account for orbit direction and incidence angle in S1 analysis\n",
      "✓ Handle cloud-affected S2 pixels appropriately\n",
      "\n",
      "============================================================\n",
      "MULTI-TEMPORAL BAND EXTRACTION COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "if not final_df.empty:\n",
    "    print(\"=== EXPORTING RESULTS ===\")\n",
    "    \n",
    "    # Export main results to CSV\n",
    "    output_filename = \"multi_temporal_satellite_bands.csv\"\n",
    "    final_df.to_csv(output_filename, index=False)\n",
    "    print(f\"✓ Main dataset exported to: {output_filename}\")\n",
    "    \n",
    "    # Create separate files for each sensor if needed\n",
    "    if 'S2' in final_df['sensor'].values:\n",
    "        s2_only = final_df[final_df['sensor'] == 'S2']\n",
    "        s2_filename = \"sentinel2_bands_multi_temporal.csv\"\n",
    "        s2_only.to_csv(s2_filename, index=False)\n",
    "        print(f\"✓ Sentinel-2 data exported to: {s2_filename}\")\n",
    "    \n",
    "    if 'S1' in final_df['sensor'].values:\n",
    "        s1_only = final_df[final_df['sensor'] == 'S1']\n",
    "        s1_filename = \"sentinel1_bands_multi_temporal.csv\"\n",
    "        s1_only.to_csv(s1_filename, index=False)\n",
    "        print(f\"✓ Sentinel-1 data exported to: {s1_filename}\")\n",
    "    \n",
    "    # Create summary report\n",
    "    print(f\"\\n=== EXTRACTION REPORT ===\")\n",
    "    print(f\"Extraction completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Total sampling points: {total_points}\")\n",
    "    print(f\"Total extractions: {len(final_df)}\")\n",
    "    print(f\"Configuration used:\")\n",
    "    for key, value in CONFIG.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Data quality summary\n",
    "    print(f\"\\n=== DATA QUALITY SUMMARY ===\")\n",
    "    \n",
    "    # Check for missing temporal slots per point\n",
    "    point_slot_counts = final_df.groupby(['point_id', 'sensor'])['slot'].nunique()\n",
    "    complete_points = point_slot_counts[point_slot_counts == 3]  # same, previous, next\n",
    "    \n",
    "    if len(complete_points) > 0:\n",
    "        print(f\"Points with complete temporal coverage: {len(complete_points)}\")\n",
    "    \n",
    "    # Cloud coverage impact (S2 only)\n",
    "    if 'S2' in final_df['sensor'].values:\n",
    "        s2_data = final_df[final_df['sensor'] == 'S2']\n",
    "        s2_band_cols = [col for col in s2_data.columns if col.startswith('s2_B')]\n",
    "        if s2_band_cols:\n",
    "            null_percentage = (s2_data[s2_band_cols].isnull().sum() / len(s2_data) * 100).round(1)\n",
    "            print(f\"\\nSentinel-2 cloud impact (% null values):\")\n",
    "            for band, pct in null_percentage.items():\n",
    "                print(f\"  {band}: {pct}%\")\n",
    "    \n",
    "    # Orbit distribution (S1 only)\n",
    "    if 'S1' in final_df['sensor'].values:\n",
    "        s1_data = final_df[final_df['sensor'] == 'S1']\n",
    "        if 's1_orbit_direction' in s1_data.columns:\n",
    "            orbit_dist = s1_data['s1_orbit_direction'].value_counts()\n",
    "            print(f\"\\nSentinel-1 orbit distribution:\")\n",
    "            for orbit, count in orbit_dist.items():\n",
    "                print(f\"  {orbit}: {count} acquisitions\")\n",
    "    \n",
    "    print(f\"\\n=== NEXT STEPS ===\")\n",
    "    print(\"✓ Raw bands extracted and ready for analysis\")\n",
    "    print(\"✓ Use extracted bands to calculate spectral indices\")\n",
    "    print(\"✓ Perform temporal analysis (same vs previous vs next)\")\n",
    "    print(\"✓ Correlate with field soil moisture measurements\")\n",
    "    print(\"✓ Account for orbit direction and incidence angle in S1 analysis\")\n",
    "    print(\"✓ Handle cloud-affected S2 pixels appropriately\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ No data to export - check extraction process\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MULTI-TEMPORAL BAND EXTRACTION COMPLETE\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c43c5",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook has successfully implemented a comprehensive multi-temporal satellite band extraction methodology:\n",
    "\n",
    "### ✅ **What Was Accomplished:**\n",
    "\n",
    "1. **Multi-temporal Approach**: Extracted data from 3 time periods per point:\n",
    "   - Same day as field measurement\n",
    "   - Previous image (within 10-day window)\n",
    "   - Next image (within 10-day window)\n",
    "\n",
    "2. **Sentinel-2 Raw Bands**: Extracted B2, B3, B4, B8, B11, B12\n",
    "   - Cloud pixels preserved as null values (no masking/replacement)\n",
    "   - Maintains data integrity for cloud impact analysis\n",
    "\n",
    "3. **Sentinel-1 with Metadata**: Extracted VV, VH, incidence angle\n",
    "   - Orbit direction (ascending/descending) included\n",
    "   - Acquisition metadata preserved\n",
    "\n",
    "4. **Structured Output**: Tidy dataset ready for:\n",
    "   - Spectral index calculation\n",
    "   - Temporal analysis\n",
    "   - Correlation with field measurements\n",
    "\n",
    "### 🎯 **Key Advantages of This Approach:**\n",
    "\n",
    "- **Date-Matched Extraction**: Ensures temporal consistency with field data\n",
    "- **Multi-temporal Context**: Enables before/after comparisons\n",
    "- **Cloud-Aware**: Preserves information about cloud contamination\n",
    "- **SAR Metadata**: Includes important radar acquisition parameters\n",
    "- **Flexible**: Easy to modify search windows and band selections\n",
    "\n",
    "### 📊 **Ready for Analysis:**\n",
    "The extracted data can now be used to:\n",
    "1. Calculate spectral indices (NDVI, NDMI, etc.)\n",
    "2. Analyze temporal patterns\n",
    "3. Correlate with soil moisture measurements\n",
    "4. Account for acquisition conditions (clouds, orbit direction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
